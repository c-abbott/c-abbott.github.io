title: Maximize your experimentation output with CUPED: a simple technique for faster A/B tests
---
hero_image: 
---
pub_date: 2023-01-15
---
body:

## Separating Signal from Noise in Product Experimentation
Recently, I have been re-positioned within my company to sit on a cross-functional product team consisting of a PM, a designer, a handful of engineers, and a behavioural scientist. We aim to boost our product's retention and engagement metrics and do it pronto. 

My role in the team is to measure the performance of the features produced by us, with the primary method of measurement being A/B testing, also known as experimentation. Now, it is no secret that data-supported decisions have ruled the roost at unicorn tech companies (Duolingo, Netflix, Airbnb) for the past decade, so it naturally follows that smaller companies try to follow suit.

However, the problem nearly all companies suffer from when running experiments is untangling the signal produced by the feature from the noise created by everyday user behaviour. Was this feature actually any good? Or are we just observing the random fluctuations that happen anyways? 

Smaller companies - with consequently smaller user bases - are hindered in this untangling process by small sample sizes, i.e. do we have enough users to power a statistically significant result out of this experiment? Meanwhile, larger companies - with larger user bases but more mature products - suffer from small effect sizes, i.e. is this 1% increase in New User Retention due to our feature, or are we just observing the variance that naturally occurs in this metric?

While grappling with the statistical challenges of online experimentation, the Data Scientist must also juggle the internal politics of making demands from an individual-contributor (IC) level. The engineers and designers understandably want their work to be shipped ASAP, and the PM often has some sort of roadmap deadlines to meet but along comes the Data Scientist with needs like:
- *We need to maintain the old feature and design and implement 2 other versions for the experiment.*
- *No, we can't ship this other feature right now since it will impact the experiment we already have running.*
- *We got fewer new users than expected this past month, so we're gonna need another couple of weeks of running the experiment.*

Online experiments undoubtedly increase product development time, but CUPED can alleviate this issue with a simple mathematical trick.

## Hypothesis testing refresher
*CUPED works by reducing the variance of the performance metric you're measuring your experiment with thus increasing statistical power.* 

If this statement makes sense to you, then you can just skip to the next section to see how CUPED is implemented in SQL + Python, but if not, then here's an intuitive explanation of hypothesis testing and t-tests.

In order to understand how CUPED works, we must first grok the math behind how one determines if a treatment effect is statistically significant. I work best with examples, so let's imagine we are a fitness SaaS company, and we want to see if our updated step counter feature causes users to walk more. We use the average number of steps each user takes during the experiment period to measure this experiment against, and we randomly expose the new step counter to 5000 users corresponding to 50% of our active step counter users.

We begin our analysis by assuming all our observations of user step counts are independent and identically distributed, which says two things:
- **Independence:** one user's average step count does not affect another. 
- **Identically Distributed:** the data-generating process governing the collection of a user's steps - also known as walking - is similar across users, and hence can be modelled with the same distribution.

With this assumption, we can then safely apply the Central Limit Theorem to derive the normality of the average step count per user metric allowing us to apply a t-test on the sample means between the treatment and control groups. Average step counts are probably normally distributed between groups anyway, but now we're doubly sure. Ploughing ahead and we compute our (Welch's) t-statistic[^1] under the null hypothesis that the sample means between the treatment and control groups are equal with the following formula:

$$\begin{aligned}t = \frac{\bar{y}_T- \bar{y}_C}{\sqrt{\frac{s_T^2}{n_T} + \frac{s_C^2}{n_C}}}
\end{aligned}$$

Where:
- $\bar{y}_{T, C}$ is the sample mean of the (treatment, control) group
- $s_{T, C}^2$ is the sample variance of the (treatment, control) group
- $n_{T, C}$ is the sample size of the (treatment, control) group

You could take my word that larger values of $t$ correspond to smaller p-values and hence greater certainty in the rejection of our null hypothesis that $
\bar{y}_T - \bar{y}_C = 0$, but if there's one thing a theoretical physics degree teaches you, it is to develop an intuition around important formulas.

So, glancing at the numerator of $t$, we see this corresponds to the effect size of the feature whilst the denominator is some measure of the variance of $\bar{y}_T - \bar{y}_C$. Larger values of $t$ are hence obtained when the feature's effect size is large and/or if the metric we use to measure the feature has a relatively low variance. In other words, it's easy to detect the effect of a feature on a product if that effect is very large, or even if the effect is very small, if the metric we're measuring success with tends to stay at similar values over time then even small changes in this metric can be attributed to the feature.

For the mathematically inclined, another way to wrap your head around this formula is to define a new random variable, $Z$, as the difference between the treatment and control average step counts per user:

$$\begin{aligned}Z = Y_T - Y_C \end{aligned}$$

Since we've already assumed $Y_T$ and $Y_C$ to be normal, $Z$ **<a href="https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables#:~:text=This%20means%20that%20the%20sum,squares%20of%20the%20standard%20deviations).">must also be normal with the following parameterization</a>**:


$$\begin{aligned} Z  \sim \mathcal{N}(\mu_{Z}, \sigma^{2}) \end{aligned}$$


Now, the parameters of $Z$'s distribution are also random variables themselves, so we must estimate them using the sampled data we've collected from our experiment. The diagram below details the formulas used to estimate these variables, which demonstrates that the numerator of $t$ is equal to our estimate of $\mu_{Z} \approx \bar{y}_T - \bar{y}_C $, and the denominator is equal to our estimate of $\sigma \approx \sqrt{\frac{s_T^2}{n_T} + \frac{s_C^2}{n_C}} = s$ - also known as the sample standard deviation of $Z$ 

<div class="caption">
  <img src="./distributions.jpeg"style="width:90%" style="height:90%">
</div>

Plotting the distribution of $Z$ on some arbitrary axes shown below, we observe that the statistical power of an experiment boils down to a tug-of-war between effect size and variance. We need the effect size of the experiment to be sufficiently large such that an insignificant amount of $Z$'s probability density sits over 0 and this notion is what the p-value quantifies. The smaller the p-value, the less this distribution sits over 0.


<div class="caption">
  <img src="./effect_sizes.jpeg"style="width:90%" style="height:90%">
</div>


So can we start getting to the point? How do I start running my experiments faster?

The formula for the t-statistic reveals there are 3 ways to obtain more significant results from experiments:

1. Larger effect sizes
2. Larger sample sizes
3. Lower variance metrics



[^1]: Delacre et al. (2017) show that Welch's t-test provides better control of Type 1 error rates when the assumption of homogeneity of variance is not met, and it loses little robustness compared to Student's t-test when the assumptions are met.

