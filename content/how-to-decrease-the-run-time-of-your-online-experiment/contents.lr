title: Maximize your experimentation output with CUPED: a simple technique for faster A/B tests
---
hero_image: 
---
pub_date: 2023-01-15
---
body:

## Separating Signal from Noise in Product Experimentation
Recently, I have been re-positioned within my company to sit on a cross-functional product team consisting of a PM, a designer, a handful of engineers, and a behavioural scientist. We aim to boost our product's retention and engagement metrics and do it pronto. 

My role in the team is to measure the performance of the features produced by us, with the primary method of measurement being A/B testing, also known as experimentation. Now, it is no secret that data-supported decisions have ruled the roost at unicorn tech companies (Duolingo, Netflix, Airbnb) for the past decade, so it naturally follows that smaller companies try to follow suit.

However, the problem nearly all companies suffer from when running experiments is untangling the signal produced by the feature from the noise created by everyday user behaviour. Was this feature actually any good? Or are we just observing the random fluctuations that happen anyways? 

Smaller companies - with consequently smaller user bases - are hindered in this untangling process by small sample sizes, i.e. do we have enough users to power a statistically significant result out of this experiment? Meanwhile, larger companies - with larger user bases but more mature products - suffer from small effect sizes, i.e. is this 1% increase in New User Retention due to my feature, or is it just the variance that naturally exists in this metric?

While grappling with the statistical challenges of online experimentation, the Data Scientist must also juggle the internal politics of making demands from an individual-contributor (IC) level. The engineers and designers understandably want their work to be shipped ASAP, and the PM often has some sort of roadmap deadlines to meet but along comes the Data Scientist with needs like:
- *We need to maintain the old feature and design and implement 2 other versions for the experiment.*
- *No, we can't ship this other feature right now since it will impact the experiment we already have running.*
- *We got fewer new users than expected this past month, so we're gonna need another couple of weeks of running the experiment.*

Online experiments undoubtedly increase product development time, but CUPED can ease this with a simple mathematical trick.

## Example: does our new step counter increase the steps taken by users?
To understand how CUPED works, we must first grok the math behind how one determines if a treatment effect is statistically significant. I work best with examples, so let's imagine we are a fitness SaaS company, and we want to see if our updated step counter feature causes users to walk more. We use the average number of steps each user takes during the experiment period to measure this experiment against, and we randomly expose the new step counter to 5000 users corresponding to 50% of our current user base.

We begin our analysis by assuming all our observations of user step counts are independent and identically distributed, which says two things:
Independence: one user's step count does not affect another. 
Identically Distributed: the data-generating process governing the collection of a user's steps - also known as walking - is similar across users and can be modelled with the same distribution.

With this assumption, we can then safely apply the Central Limit Theorem to derive the normality of the average step count per user metric, enabling us to apply our beloved t-test on the sample means between the treatment and control groups. Average step counts are probably normally distributed between groups anyway, but now we're doubly sure. We, therefore, plough ahead and compute our (Welch's) t-statistic under the null hypothesis that the sample means between the treatment and control groups are equal with the following formula:

$$\begin{aligned}t = \frac{\bar{x}_T- \bar{x}_C}{\sqrt{\frac{s_T^2}{n_T} + \frac{s_C^2}{n_C}}}
\end{aligned}$$

Where:
- $\bar{x}_T$ is the sample mean of the treatment group
- $\bar{x}_C$ is the sample mean of the control group
- $s_T^2$ is the sample variance of the treatment group
- $s_C^2$ is the sample variance of the control group
- $n_T$ is the sample size of the treatment group
- $n_C$ is the sample size of the control group

Delacre et al. (2017) show that Welch's t-test provides better control of Type 1 error rates when the assumption of homogeneity of variance is not met, and it loses little robustness compared to Student's t-test when the assumptions are met.

Glancing at this formula intuitively, we can see that the numerator of this formula corresponds to the effect size of the feature - the difference in sample means between the treatment and control groups - so larger effect sizes correspond to larger values of t, which result in smaller p-values. In other words, assuming constant variance in the metric, the larger its change, the easier it is to determine that a change in a metric was statistically significant. Makes sense. Alternatively, let's investigate the denominator of this formula under the assumption of a constant effect size. We see that metrics with larger variances result in a reduction in the value of the t-statistic, thus creating larger, less significant p-values. This also makes sense since even if you have a large effect size, if the variance of this metric is also relatively large, then it is difficult to determine whether this change is actually due to the feature you just shipped or is just noise.

So can we start getting to the point? How do I start running my experiments faster?

Rearranging the formula for the t-statistic, we observe that there are three ways we can try to yield more significant results from our experiments allowing us to end them faster:

1. Larger effect sizes: ship better features
2. Larger sample sizes: run experiments for longer
3. Lower variance metrics (and hence lower standard deviations in the metric): CUPED

It's clearly difficult for the Data Scientist to affect point 1. since they are usually not the ones producing the feature. However, this notion can be used as an argument against drawing conclusions from experiments run on half-baked features. Point 2. is exactly the scenario we're trying to avoid in this blog post, which leaves us with point 3. of choosing lower variance metrics to measure experiments against over higher variance metrics. Whilst this 






the common scenario where we have been tasked by the team to decide whether our new push notification strategy has increased 7-Day Retention for our product.

The new push strategy was randomly exposed to 5000 out of our latest 10,000 new users, meaning we have valid treatment and control groups to compare.

Since 7-Day Retention is a binary variable at the user level, it makes sense to model this data-generating process as a binomial distribution with $N$ trials and a probability of retention, $p$. However, given the volume of data we have, we can safely apply the Central Limit Theorem (CLT) to approximate this binomial distribution as a normal distribution with mean $p$ and variance $\frac{p(1-p)}{N}$ . To prove this, l

