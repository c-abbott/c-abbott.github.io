title: Maximize your experimentation output with CUPED: a simple technique for faster A/B tests
---
hero_image: 
---
pub_date: 2023-01-15
---
body:

## Separating Signal from Noise in Product Experimentation
Recently, I have been re-positioned within my company to sit on a cross-functional product team consisting of a PM, a designer, a handful of engineers, and a behavioural scientist. We aim to boost our product's retention and engagement metrics and do it pronto. 

My role in the team is to measure the performance of the features produced by us, with the primary method of measurement being A/B testing, also known as experimentation. Now, it is no secret that data-supported decisions have ruled the roost at unicorn tech companies (Duolingo, Netflix, Airbnb) for the past decade, so it naturally follows that smaller companies try to follow suit.

However, the problem nearly all companies suffer from when running experiments is untangling the signal produced by the feature from the noise created by everyday user behaviour. Was this feature actually any good? Or are we just observing the random fluctuations that happen anyways? 

Smaller companies - with consequently smaller user bases - are hindered in this untangling process by small sample sizes, i.e. do we have enough users to power a statistically significant result out of this experiment? Meanwhile, larger companies - with larger user bases but more mature products - suffer from small effect sizes, i.e. is this 1% increase in New User Retention due to my feature, or is it just the variance that naturally exists in this metric?

While grappling with the statistical challenges of online experimentation, the Data Scientist must also juggle the internal politics of making demands from an individual-contributor (IC) level. The engineers and designers understandably want their work to be shipped ASAP, and the PM often has some sort of roadmap deadlines to meet but along comes the Data Scientist with needs like:
- *We need to maintain the old feature and design and implement 2 other versions for the experiment.*
- *No, we can't ship this other feature right now since it will impact the experiment we already have running.*
- *We got fewer new users than expected this past month, so we're gonna need another couple of weeks of running the experiment.*

Online experiments undoubtedly increase product development time, but CUPED can ease this with a simple mathematical trick.

## Example: does our new step counter increase the steps taken by users?
To understand how CUPED works, we must first grok the math behind how one determines if a treatment effect is statistically significant. I work best with examples, so let's imagine we are a fitness SaaS company, and we want to see if our updated step counter feature causes users to walk more. We use the average number of steps each user takes during the experiment period to measure this experiment against, and we randomly expose the new step counter to 5000 users corresponding to 50% of our current user base.

We begin our analysis by assuming all our observations of user step counts are independent and identically distributed, which says two things:
Independence: one user's step count does not affect another. 
Identically Distributed: the data-generating process governing the collection of a user's steps - also known as walking - is similar across users and can be modelled with the same distribution.

With this assumption, we can then safely apply the Central Limit Theorem to derive the normality of the average step count per user metric, enabling us to apply our beloved t-test on the sample means between the treatment and control groups. Average step counts are probably normally distributed between groups anyway but now we're doubly sure :)

the common scenario where we have been tasked by the team to decide whether our new push notification strategy has increased 7-Day Retention for our product.

The new push strategy was randomly exposed to 5000 out of our latest 10,000 new users, meaning we have valid treatment and control groups to compare.

Since 7-Day Retention is a binary variable at the user level, it makes sense to model this data-generating process as a binomial distribution with $N$ trials and a probability of retention, $p$. However, given the volume of data we have, we can safely apply the Central Limit Theorem (CLT) to approximate this binomial distribution as a normal distribution with mean $p$ and variance $\frac{p(1-p)}{N}$ . To prove this, l

