title: the flow of causation
---
hero_image:
---
pub_date: 2021-05-24
---
body:

## mapping causal graphs to probability distributions
Now that we are familiar with the concept of DAGs and causal graphs, we can begin to analyze these mathematical objects with a little more rigour. Taking the causal graph depicted in Fig. 1 below as an example, it turns out by constructing this graph we are implicitly modelling a probability distribution over the set of four random variables $\\{A, B, C, D\\}$. This probability distribution is known as a **<a href ="https://en.wikipedia.org/wiki/Joint_probability_distribution"> joint probability distribution,</a>** and *na√Øvely* takes the following form: 

$$\begin{aligned}p(A, B, C, D) = p(A) p(B \mid A) p(C \mid A,B) \color{BurntOrange}p(D \mid A, B, C)\end{aligned}$$

However, given the information encoded in our graph, we can simplify the expression by taking note of which variables are causally connected. For example, the quantity $p(D \mid A, B, C)$ denotes the probability distribution over the random variable, $D$, conditional on random variables $A$, $B$, and $C$ -- but do we really need to condition on all 3 of these variables? Fig. 1 illustrates that only $C$ is a direct cause of $D$, and so actually only $C$ needs to be conditioned on in order to encapsulate the distribution of $D$. Mathematically speaking, $D$ is said to be independent of $A$ and $B$, simplifying our expression of the joint distribution to:
$$D\perp\kern-5pt\perp\\{A, B\\}$$

$$\implies \begin{aligned}p(A, B, C, D) = p(A) p(B \mid A) p(C \mid A,B) \color{BurntOrange}p(D \mid C)\end{aligned}$$

<div class="caption">
  <img src="./markov-assump.png"style="width:70%" style="height:70%" class="shadow-img">
  <p> Figure 1</p>
</div>
This intuitive analysis of the causal graph in Fig. 1 actually embodies one of the cornerstone assumptions in the analysis of causal graphs known as *Minimality*.

**Minimality Assumption**

1. A random variable $X$ in a DAG $\mathcal{G}$ is independent of all its non-descendants i.e. $\\{A,B\\}$ are the non-descendants of $D$.

2. Adjacent variables/nodes ($C$ and $D$) in a DAG $\mathcal{G}$ are dependent.

The first bullet point of the minimality assumption is more formally known as the **<a href="https://en.wikipedia.org/wiki/Markov_property">Local Markov Assumption</a>**, or the **<a href="https://en.wikipedia.org/wiki/Markov_property">Markov Property</a>**. It is a simplification that is often made when modelling stochastic processes, making it commonly utilized in subjects such as statistical mechanics, reinforcement learning, and statistics. Generally speaking, processes which are assumed to obey this property are colloquially known as *memory-less*, since the current state of the system (a particular node in the DAG) is assumed to be dependent on only the previous state (direct cause). The main consequence of the Local Markov Assumption is the **<a href="https://en.wikipedia.org/wiki/Bayesian_network">Bayesian Network Factorization</a>** which is outlined as follows:

**Bayesian Network Factorization**

- Given a probability distribution $p$ and a DAG $\mathcal{G}$, $p$ factorizes according to $\mathcal{G}$ if
$$p(x_1,...,x_n) = \prod_ip(x_i\mid\text{pa}_i)$$
where $\text{pa}_i$ denotes the parents (direct causes) and node (variable) $x_i$.

This notion was illustrated explicitly when we performed the simplification: $$\color{BurntOrange}p(D\mid A,B,C) \rightarrow p(D\mid C)$$

I appreciate that mathematical jargon such as the Markov Property, Bayesian Network Factorization, and Minimality may only add confusion to my explanations of causal graphs, however, I would encourage readers with lesser experience in mathematics to try look past the jargon, and connect with what the concept is actually addressing at heart -- the notion addressed are often rather intuitive and easy to understand when explained with words. 

## causal graph building blocks

## conditioning on chains and forks

## conditioning on colliders

## intervening vs. conditions

## backdoor criterion
